{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344d1043-9d5d-4367-8876-2be867c3e935",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "imports"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Workspace/Repos/js-julian.salinas@outlook.com/aquiles-etl-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39f058d4-461e-43ba-93b2-30855be1896b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "initialize"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DateTransformation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9170139e-fc00-451d-aa70-5c3455da081a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read table"
    }
   },
   "outputs": [],
   "source": [
    "# Read the table into a DataFrame\n",
    "df = spark.read.table(\"workspace.default.products_from_csv\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0f91ed-06ee-4c7d-ba4d-61915fed8b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, initcap, when\n",
    "from pyspark.sql.types import DecimalType, StringType, StructField, StructType\n",
    "from core.data_processor import infer_and_transform_date, transform_price, remove_special_characters, separate_camel_case, transform_provider_name, extract_measure_and_unit\n",
    "\n",
    "\n",
    "# Define the schema for the returned STRUCT type\n",
    "measure_unit_schema = StructType([\n",
    "    StructField(\"Measure\", StringType(), True),\n",
    "    StructField(\"UnitOfMeasure\", StringType(), True),\n",
    "    StructField(\"PackageUnits\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Register the UDFs\n",
    "infer_and_transform_date_udf = udf(infer_and_transform_date, StringType())\n",
    "transform_price_udf = udf(transform_price, DecimalType())\n",
    "remove_special_characters_udf = udf(remove_special_characters, StringType())\n",
    "separate_camel_case_udf = udf(separate_camel_case, StringType())\n",
    "transform_provider_name_udf = udf(transform_provider_name, StringType())\n",
    "extract_measure_and_unit_udf = udf(extract_measure_and_unit, measure_unit_schema)\n",
    "\n",
    "# Convert data types using the UDFs\n",
    "df = df.withColumn(\"RawPrice\", col(\"Price\")) \\\n",
    "       .withColumn(\"CleanPrice\", transform_price_udf(col(\"Price\"))) \\\n",
    "       .withColumn(\"RawLastReviewDt\", col(\"LastReviewDt\")) \\\n",
    "       .withColumn(\"CleanLastReviewDt\", infer_and_transform_date_udf(col(\"LastReviewDt\"))) \\\n",
    "       .withColumn(\"RawDescription\", col(\"Description\")) \\\n",
    "       .withColumn(\"CleanDescription\", remove_special_characters_udf(col(\"Description\"))) \\\n",
    "       .withColumn(\"CleanProviderName\", initcap(transform_provider_name_udf(col(\"ProviderName\")))) \\\n",
    "       .withColumn(\"IsValidPrice\", when(col(\"Price\").isNull(), False).otherwise(True))\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c027869-a98b-4555-abff-7df81c05df15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "# Convert data types using the UDFs\n",
    "df = df.withColumn(\"MeasureUnit\", extract_measure_and_unit_udf(col(\"Description\"))) \\\n",
    "       .withColumn(\"Measure\", col(\"MeasureUnit.Measure\")) \\\n",
    "       .withColumn(\"UnitOfMeasure\", col(\"MeasureUnit.UnitOfMeasure\")) \\\n",
    "       .withColumn(\"PackageUnits\", col(\"MeasureUnit.PackageUnits\")) \\\n",
    "       .withColumn(\"UnitOfMeasure\", lower(col(\"UnitOfMeasure\")))\n",
    "\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "clean-data-for-oltp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
