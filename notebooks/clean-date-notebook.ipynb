{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d12f8b77-7841-4a65-8ef1-2f09c2e03bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39f058d4-461e-43ba-93b2-30855be1896b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "initialize"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DateTransformation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb3c6fc-1bd4-441b-80af-831c4c749ff5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753479044104}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, udf, expr, when, initcap, lower\n",
    "from pyspark.sql.types import DecimalType, StringType, IntegerType, FloatType, StructType, StructField\n",
    "from dateutil.parser import parse\n",
    "from common.transforms import infer_and_transform_date, transform_price, remove_special_characters, separate_camel_case, transform_provider_name\n",
    "import re\n",
    "\n",
    "# Register the UDFs\n",
    "infer_and_transform_date_udf = udf(infer_and_transform_date, StringType())\n",
    "transform_price_udf = udf(transform_price, DecimalType())\n",
    "remove_special_characters_udf = udf(remove_special_characters, StringType())\n",
    "separate_camel_case_udf = udf(separate_camel_case, StringType())\n",
    "transform_provider_name_udf = udf(transform_provider_name, StringType())\n",
    "\n",
    "# Read the table into a DataFrame\n",
    "df = spark.read.table(\"workspace.default.products_from_csv\")\n",
    "\n",
    "# Convert data types using the UDFs\n",
    "df = df.withColumn(\"RawPrice\", col(\"Price\")) \\\n",
    "       .withColumn(\"Price\", transform_price_udf(col(\"Price\"))) \\\n",
    "       .withColumn(\"RawLastReviewDt\", col(\"LastReviewDt\")) \\\n",
    "       .withColumn(\"LastReviewDt\", infer_and_transform_date_udf(col(\"LastReviewDt\"))) \\\n",
    "       .withColumn(\"RawDescription\", col(\"Description\")) \\\n",
    "       .withColumn(\"Description\", remove_special_characters_udf(col(\"Description\"))) \\\n",
    "       .withColumn(\"ProviderName\", initcap(transform_provider_name_udf(col(\"ProviderName\")))) \\\n",
    "       .withColumn(\"IsValidPrice\", when(col(\"Price\").isNull(), False).otherwise(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fbbf7d6-01bb-4182-8018-7060cf431de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, udf, expr, when, initcap, lower\n",
    "from pyspark.sql.types import DecimalType, StringType, IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.sql import SparkSession\n",
    "from dateutil.parser import parse\n",
    "from decimal import *\n",
    "import re\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DateTransformation\").getOrCreate()\n",
    "\n",
    "# Define a function to infer the date format and transform the date string\n",
    "def infer_and_transform_date(date_str):\n",
    "    try:\n",
    "        parsed_date = parse(date_str, fuzzy=True)\n",
    "        return parsed_date.strftime(\"%Y-%m-%d\")\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Define a function to transform the Price column\n",
    "def transform_price(price_str):\n",
    "    try:\n",
    "        cleaned_price_str = price_str.replace(\".\", \"\").replace(\",\", \"\").replace(\"$\", \"\").replace(\" \", \"\")\n",
    "        return Decimal(cleaned_price_str)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Define a function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    try:\n",
    "        return re.sub(r'[^A-Za-z0-9/ ]+', '', text)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Define a function to separate camel case\n",
    "def separate_camel_case(text):\n",
    "    try:\n",
    "        return re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Define a function to transform the ProviderName column\n",
    "def transform_provider_name(provider_name):\n",
    "    try:\n",
    "        cleaned_name = remove_special_characters(provider_name)\n",
    "        separated_name = separate_camel_case(cleaned_name)\n",
    "        return separated_name\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Define a function to extract the unit of measure and the measure from a given string\n",
    "def extract_measure_and_unit(measure_str):\n",
    "    try:\n",
    "        measure = re.findall(r\"(\\d+\\.?\\d*)\\s*([a-zA-Z]{1,2})\", measure_str)\n",
    "        packageUnits =  re.findall(r\"[x]\\s*(\\d+)\", measure_str)\n",
    "        return (measure[0][0] if measure else None, measure[0][1] if measure else None, packageUnits[0] if packageUnits else None)\n",
    "    except Exception as e:\n",
    "        return (None, None, None)\n",
    "\n",
    "# Define the schema for the returned STRUCT type\n",
    "measure_unit_schema = StructType([\n",
    "    StructField(\"Measure\", StringType(), True),\n",
    "    StructField(\"UnitOfMeasure\", StringType(), True),\n",
    "    StructField(\"PackageUnits\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Register the UDFs\n",
    "infer_and_transform_date_udf = udf(infer_and_transform_date, StringType())\n",
    "transform_price_udf = udf(transform_price, DecimalType())\n",
    "remove_special_characters_udf = udf(remove_special_characters, StringType())\n",
    "separate_camel_case_udf = udf(separate_camel_case, StringType())\n",
    "transform_provider_name_udf = udf(transform_provider_name, StringType())\n",
    "extract_measure_and_unit_udf = udf(extract_measure_and_unit, measure_unit_schema)\n",
    "\n",
    "# Read the table into a DataFrame\n",
    "df = spark.read.table(\"workspace.default.products_from_csv\")\n",
    "\n",
    "# Convert data types using the UDFs\n",
    "df = df.withColumn(\"RawPrice\", col(\"Price\")) \\\n",
    "       .withColumn(\"Price\", transform_price_udf(col(\"Price\"))) \\\n",
    "       .withColumn(\"RawLastReviewDt\", col(\"LastReviewDt\")) \\\n",
    "       .withColumn(\"LastReviewDt\", infer_and_transform_date_udf(col(\"LastReviewDt\"))) \\\n",
    "       .withColumn(\"RawDescription\", col(\"Description\")) \\\n",
    "       .withColumn(\"Description\", remove_special_characters_udf(col(\"Description\"))) \\\n",
    "       .withColumn(\"ProviderName\", initcap(transform_provider_name_udf(col(\"ProviderName\")))) \\\n",
    "       .withColumn(\"IsValidPrice\", when(col(\"Price\").isNull(), False).otherwise(True)) \\\n",
    "       .withColumn(\"MeasureUnit\", extract_measure_and_unit_udf(col(\"Description\"))) \\\n",
    "       .withColumn(\"Measure\", col(\"MeasureUnit.Measure\")) \\\n",
    "       .withColumn(\"UnitOfMeasure\", col(\"MeasureUnit.UnitOfMeasure\")) \\\n",
    "       .withColumn(\"PackageUnits\", col(\"MeasureUnit.PackageUnits\")) \\\n",
    "       .withColumn(\"UnitOfMeasure\", lower(col(\"UnitOfMeasure\")))\n",
    "\n",
    "# Display the DataFrame\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc99b5bb-d624-457e-817a-92652f01785f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753572082739}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "timezones = spark.createDataFrame(pytz.all_timezones, [\"Timezone\"])\n",
    "\n",
    "timezones.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "clean-date-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
